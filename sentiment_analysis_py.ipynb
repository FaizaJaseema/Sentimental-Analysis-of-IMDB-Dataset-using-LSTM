{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMZObaeHxvxMsWxgBhRz6h2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FaizaJaseema/Sentimental-Analysis-of-IMDB-Dataset-using-LSTM/blob/main/sentiment_analysis_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries\n"
      ],
      "metadata": {
        "id": "NLqjoHhKJbN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "7jw3G5cmJc5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data set adding"
      ],
      "metadata": {
        "id": "SnKgK5fjKub3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset (adjust the path as needed)\n",
        "df = pd.read_csv('a1_IMDB_Dataset.csv')  # Replace with the correct path\n",
        "\n",
        "# Ensure the dataset has 'review' and 'sentiment' columns\n",
        "print(df.head(10))\n",
        "\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts(df['review'].values)\n",
        "X = tokenizer.texts_to_sequences(df['review'].values)\n",
        "X = pad_sequences(X)\n"
      ],
      "metadata": {
        "id": "xlGD1ZLXKyxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ],
      "metadata": {
        "id": "P84nB4WFKz0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Input\n",
        "embed_dim = 64\n",
        "lstm_out = 16\n",
        "max_features = 2000  # Vocabulary size\n",
        "\n",
        "# Assuming X.shape[1] is the sequence length (1939 based on the model summary)\n",
        "input_length = X.shape[1]\n",
        "\n",
        "# Build the model using Input() for the input layer\n",
        "model = Sequential()\n",
        "\n",
        "# Specify input shape using Input() at the start of the Sequential model\n",
        "model.add(Input(shape=(input_length,)))\n",
        "\n",
        "# Add Embedding layer\n",
        "model.add(Embedding(max_features, embed_dim))\n",
        "\n",
        "# Add LSTM layer\n",
        "model.add(LSTM(lstm_out))\n",
        "\n",
        "# Add Dense layer with sigmoid activation\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "j9JRioDcK0Zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "NvDHQR-nQDLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df['sentiment'].values"
      ],
      "metadata": {
        "id": "tqJsxBt7QDeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test Shape"
      ],
      "metadata": {
        "id": "cft2ptWmQQOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.1)\n",
        "print(X_train.shape,Y_train.shape)\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "metadata": {
        "id": "mc7P6VvEQQhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "ud27Xn83QYhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
        "\n",
        "# Define the model\n",
        "lstm_model = Sequential()\n",
        "\n",
        "# Define the input shape explicitly using Input() layer\n",
        "lstm_model.add(Input(shape=(maxlen,)))\n",
        "\n",
        "# Add the embedding layer (without input_length or input_dim, as it's inferred from the Input layer)\n",
        "embedding_layer = Embedding(input_dim=vocab_length,\n",
        "                            output_dim=100,\n",
        "                            weights=[embedding_matrix],\n",
        "                            trainable=False)\n",
        "\n",
        "# Add embedding layer to the model\n",
        "lstm_model.add(embedding_layer)\n",
        "\n",
        "# Add LSTM layer\n",
        "lstm_model.add(LSTM(128))\n",
        "\n",
        "# Add Dense output layer\n",
        "lstm_model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Print the model summary to check if it's built correctly\n",
        "print(lstm_model.summary())\n",
        "\n",
        "lstm_model_history = lstm_model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)\n",
        "score = lstm_model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"Test Score:\", score[0])\n",
        "print(\"Test Accuracy:\", score[1])\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(lstm_model_history.history['acc'])\n",
        "plt.plot(lstm_model_history.history['val_acc'])\n",
        "\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(lstm_model_history.history['loss'])\n",
        "plt.plot(lstm_model_history.history['val_loss'])\n",
        "\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xcy9DMMl8rxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_nn_train = lstm_model.predict(X_train_padded)\n",
        "predictions_nn_test = lstm_model.predict(X_test_padded)\n",
        "\n",
        "predictions_nn_train = (predictions_nn_train > 0.5).astype(int)\n",
        "predictions_nn_test = (predictions_nn_test > 0.5).astype(int)\n",
        "\n",
        "# Calculate accuracy\n",
        "train_accuracy = accuracy_score(Y_train, predictions_nn_train)\n",
        "test_accuracy = accuracy_score(Y_test, predictions_nn_test)\n",
        "\n",
        "print('Train accuracy:', train_accuracy)\n",
        "print('Test accuracy:', test_accuracy)\n"
      ],
      "metadata": {
        "id": "-p-oUlDG5xlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion Matrix"
      ],
      "metadata": {
        "id": "Zrf0PqUWSRX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#  Generate Confusion Matrix\n",
        "# Training Set\n",
        "cm_train = confusion_matrix(Y_train, predictions_nn_train)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Confusion Matrix for LSTM - Train Set')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "# Testing Set\n",
        "cm_test = confusion_matrix(Y_test, predictions_nn_test)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Confusion Matrix for LSTM - Test Set')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "63X89ZLom5LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Reverse the dictionary to map indices to words (for faster lookup)\n",
        "reverse_dictionary = {val: key for key, val in word_tokenizer.word_index.items()}\n",
        "\n",
        "# Initialize a list to store the sentences\n",
        "sentences = []\n",
        "\n",
        "# Iterate through tokenized X_test and reconstruct sentences\n",
        "for j in range(len(X_test_padded)):\n",
        "    # Rebuild the sentence by mapping each token to its word using the reverse_dictionary\n",
        "    sentence = [reverse_dictionary.get(X_test_padded[j][i], '') for i in range(len(X_test_padded[j])) if X_test_padded[j][i] != 0]\n",
        "    sentences.append(' '.join(sentence))  # Join tokens to form the sentence\n",
        "\n",
        "# Assuming `predictions_nn_test` is the predictions from the neural network\n",
        "# Ensure predictions are a numpy array\n",
        "predictions_nn_test = np.array(predictions_nn_test)\n",
        "\n",
        "# Print the shape of predictions to check for any mismatches\n",
        "print(\"Shape of predictions_nn_test:\", predictions_nn_test.shape)\n",
        "\n",
        "# Check if the size matches the number of rows in Y_test and reshape if necessary\n",
        "if predictions_nn_test.shape[0] == len(Y_test):\n",
        "    # Reshape predictions only if it matches the size of Y_test\n",
        "    predictions_nn_test = predictions_nn_test.reshape(len(Y_test),)\n",
        "else:\n",
        "    print(f\"Cannot reshape predictions_nn_test to ({len(Y_test)},). Current shape: {predictions_nn_test.shape}\")\n",
        "\n",
        "# Create the error analysis DataFrame\n",
        "err_analysis = pd.DataFrame({\n",
        "    'sentences': sentences,\n",
        "    'y_true': Y_test,\n",
        "    'y_pred': predictions_nn_test\n",
        "})\n",
        "\n",
        "# Display the first 20 rows of the DataFrame for error analysis\n",
        "print(err_analysis.head(20))\n"
      ],
      "metadata": {
        "id": "DLNSItBk7knm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error Analysis"
      ],
      "metadata": {
        "id": "4O2fEEn6ShzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "errors = err_analysis.loc[err_analysis['y_pred']!=err_analysis['y_true']]\n",
        "errors.head(8)"
      ],
      "metadata": {
        "id": "UaQ1jZ-5SiVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final output"
      ],
      "metadata": {
        "id": "NNDdpuC2SoE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('a1_IMDB_Dataset.csv')\n",
        "\n",
        "df_neg = df[ df['sentiment'] == 'positive']\n",
        "df_pos = df[df['sentiment'] == 'negative']\n",
        "\n",
        "all_count_pos = len(df_pos)\n",
        "all_count_neg = len(df_neg)\n",
        "print('Count positives: ', all_count_pos)\n",
        "print('Count negatives: ', all_count_neg)\n",
        "err_count_pos = len(errors[ errors['y_true'] == 1])\n",
        "err_count_neg = len(errors[ errors['y_true'] == 0])\n",
        "print('Errors in true positive: ', err_count_pos)\n",
        "print('Errors in true negative: ', err_count_neg)\n",
        "print('Fraction of the errors with true positive:', round(err_count_pos/all_count_pos, 4))\n",
        "print('Fraction of the errors with true negative:', round(err_count_neg/all_count_neg, 4))\n"
      ],
      "metadata": {
        "id": "VWk7QiAjSogx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}